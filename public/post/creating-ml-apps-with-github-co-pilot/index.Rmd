---
title: Github Co-pilot - pros and cons
author: Rishabh Kumar
date: '2021-11-21'
slug: creating-ml-apps-with-github-co-pilot
categories:
  - ML
  - AI
  - Automation
  - NLP
  - Data Science
tags:
  - AI
  - analysis
  - Models
  - Machine Learning
  - Automation
comments: yes
images: ~
---

Recently I got hold of a beta version of GitHub co-pilot. [GitHub Copilot](https://copilot.github.com) is an AI pair programmer that helps you write code faster and with less work. GitHub Copilot draws context from comments and code, and suggests individual lines and whole functions instantly. GitHub Copilot is powered by OpenAI Codex, a new AI system created by OpenAI. 

It really feels quite magical to use. please see the gif below.

## How good is it?
According to Github: The model got their requests right about 43% of the time on the first try, and 57% of the time when allowed 10 attempts. And it’s getting smarter all the time. In my experience it was right about 70% right when allowed just 2 attempts.

## This looks super cool:


![](/post/2021-11-21-creating-ml-apps-with-github-co-pilot_files/My-Movie-1.gif)

## How is Copilot created

Copilot is powered by a deep neural network language model called Codex,a language model based on [GPT3](https://en.wikipedia.org/wiki/GPT-3), that has been trained on massive amounts of open source code from GitHub. 

A language model is trained to guess missing words in a piece of text. The traditional “ngram” approach used in previous years can not do a good job of this, since context is required to guess correctly. Imagine you have two sentences:

- I want to eat hot ______
- Today was a hot _____

Knowing that in one case “hot day” is correct, but also knowing in another that “hot food” is correct, requires reading and (to some extent) understanding the whole sentence. The Codex language model learns to guess missing symbols in programming code, so it has to learn a lot about the structure and meaning of computer code. This ability is a feature of foundationals language models like BERT and GPTs.

## Pros and cons

### Pros
- You can build a MVP in a matter of hours instead of days
- You get exposed to new libraries and methods
- You get a new perspective to what you were thinking


### Cons 
- There could be [security](https://arxiv.org/pdf/2108.09293.pdf) bugs in the code
- The code you get isn't the most optimal or efficient
- Programmers might rely too much on computer's suggestion
- AI Bias can creep in if programmers are not careful
- Human cognitive biases can also creep in. Namely: [automation bias](https://www.paconsulting.com/insights/what-is-automation-bias-how-to-prevent/) and [anchoring bias](https://thedecisionlab.com/biases/anchoring-bias/)

## Automation taking automator's job?
I think not. Coding is not a manual process like assembling cars. Nevertheless there are some similarities - for example if you were to classify text there are some common steps (generating word embeddings); still coding requires  creativity and flexibility from the coder. Having a tool like this would be having an assistant like R2D2 from Star Wars. The co-pilot can give you very useful suggestions but they would need the the pilot's creativity to make an impact.

# Where can we use this?

Currently this tool can be very useful for companies that are trying to train their staff in digital skills. Having an exposure to how code is written will help people gain an understanding. The interface itself is really cool and feels like magic. This should be leveraged to entice people who are required to know more about coding.




